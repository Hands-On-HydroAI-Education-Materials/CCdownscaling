{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cdf392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section 0: imports and setup\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import xarray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from ccdown import correction_downscale_methods, distribution_tests, error_metrics, som_downscale, utilities, \\\n",
    "    train_test_splits, climdex, plotters\n",
    "# for reproducibility\n",
    "seed = 1\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799561bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section 1: loading and organizing the data\n",
    "#define the inputs\n",
    "downscaling_target='precip'\n",
    "station_id='725300-94846' \n",
    "input_vars = {'air': 850, 'rhum': 850, 'uwnd': 700, 'vwnd': 700, 'hgt': 500}\n",
    "#load data\n",
    "station_data = pd.read_csv('./data/stations/' + station_id + '.csv')\n",
    "station_data = station_data.replace(to_replace=[99.99, 9999.9], value=np.nan)\n",
    "reanalysis_data = xarray.open_mfdataset('./data/models/*NCEP*')\n",
    "stations_info = pd.read_csv('./data/stations/stations.csv')\n",
    "station_info = stations_info.loc[stations_info['stationID'] == station_id]\n",
    "station_lat = station_info['LAT'].values\n",
    "station_lon = station_info['LON'].values\n",
    "# load the reanalysis precipitation\n",
    "if downscaling_target == 'precip':\n",
    "    rean_precip = xarray.open_mfdataset('./data/reanalysis/prate_1976-2005_NCEP_midwest.nc')\n",
    "    # convert from Kg/m^2/s to mm/day\n",
    "    rean_precip['prate'] = rean_precip['prate'] * 86400\n",
    "    rean_precip = rean_precip['prate'].sel(lat=station_lat, lon=station_lon, method='nearest').values\n",
    "    rean_precip = np.squeeze(rean_precip)\n",
    "elif downscaling_target == 'max_temp':\n",
    "    rean_precip = xarray.open_mfdataset('./data/models/air_1976-2005_NCEP_midwest.nc')\n",
    "    rean_precip = rean_precip['air'].sel(level=1000, lat=station_lat, lon=station_lon, method='nearest').values\n",
    "    # convert K to C\n",
    "    rean_precip = rean_precip - 273.15\n",
    "    rean_precip = np.squeeze(rean_precip)\n",
    "\n",
    "# select the station data to match the time from of the reanalysis data\n",
    "start = reanalysis_data['time'][0].values\n",
    "end = reanalysis_data['time'][-1].values\n",
    "station_data['time'] = pd.to_datetime(station_data['date'], format='%Y-%m-%d')\n",
    "date_mask = ((station_data['time'] >= start) & (station_data['time'] <= end))\n",
    "station_data = station_data[date_mask]\n",
    "\n",
    "hist_data = station_data[downscaling_target].values\n",
    "# Convert units, F to C for temperature, in/day to mm/day for precip\n",
    "if downscaling_target == 'max_temp':\n",
    "    hist_data = (hist_data - 32) * 5 / 9\n",
    "if downscaling_target == 'precip':\n",
    "    hist_data = hist_data * 25.4\n",
    "# For just a single grid point:\n",
    "# reanalysis_data = reanalysis_data.sel(lat = station_lat, lon = station_lon, method='nearest')\n",
    "# To use multiple grid points in a window around the location:\n",
    "window = 2\n",
    "lat_index = np.argmin(np.abs(reanalysis_data['lat'].values - station_lat))\n",
    "lon_index = np.argmin(np.abs(reanalysis_data['lon'].values - station_lon))\n",
    "reanalysis_data = reanalysis_data.isel({'lat': slice(lat_index - window, lat_index + window + 1),\n",
    "                                        'lon': slice(lon_index - window, lon_index + window + 1)})\n",
    "\n",
    "# Drop leap days for ease of use:\n",
    "hist_data = utilities.remove_leap_days(hist_data, start_year=1976)\n",
    "rean_precip = utilities.remove_leap_days(rean_precip, start_year=1976)\n",
    "reanalysis_data = utilities.remove_xarray_leap_days(reanalysis_data)\n",
    "\n",
    "# split train and test sets:\n",
    "dates = reanalysis_data['time']\n",
    "hist_data = xarray.DataArray(data=hist_data, dims=['time'], coords={'time': dates})\n",
    "rean_precip = xarray.DataArray(data=rean_precip, dims=['time'], coords={'time': dates})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section 2: Splitting the train and test sets\n",
    "#This section has three options for splitting the data: simple, seasonal, and pecentile\n",
    "split_type = 'percentile'\n",
    "dates = reanalysis_data['time']\n",
    "hist_data = xarray.DataArray(data=hist_data, dims=['time'], coords={'time': dates})\n",
    "rean_precip = xarray.DataArray(data=rean_precip, dims=['time'], coords={'time': dates})\n",
    "\n",
    "# with a simple split:\n",
    "if split_type == 'simple':\n",
    "    train_data, train_hist, test_data, test_hist, rean_precip_train, rean_precip_test = train_test_splits.simple_split(\n",
    "    reanalysis_data, hist_data, rean_precip)\n",
    "\n",
    "# selecting the highest precip/temperature years:\n",
    "elif split_type == 'percentile':\n",
    "    train_data, test_data, train_hist, test_hist, rean_precip_train, rean_precip_test = train_test_splits.select_max_target_years(\n",
    "   reanalysis_data, hist_data, 'max', time_period='year', split=0.8, rean_data=rean_precip)\n",
    "\n",
    "# training on the spring, testing on the summer:\n",
    "elif split_type == 'seasonal':\n",
    "    train_dates = utilities.generate_dates_list('3/1', '5/31', list(range(1976, 2006)))\n",
    "    test_dates = utilities.generate_dates_list('6/1', '8/31', list(range(1976, 2006)))\n",
    "    train_data, test_data, train_hist, test_hist, rean_precip_train, rean_precip_test = train_test_splits.select_season_train_test(\n",
    "    \treanalysis_data, hist_data, train_dates, test_dates, rean_data=rean_precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88cc0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section 3: normalize the data, and reorganize for use in the downscaling methods\n",
    "input_train_data = []\n",
    "input_test_data = []\n",
    "for var in input_vars:\n",
    "    var_data = train_data.sel(level=input_vars[var])[var].values\n",
    "    var_data = var_data.reshape(var_data.shape[0], var_data.shape[1] * var_data.shape[2])\n",
    "    input_train_data.append(var_data)\n",
    "    var_test_data = test_data.sel(level=input_vars[var])[var].values\n",
    "    var_test_data = var_test_data.reshape(var_test_data.shape[0], var_test_data.shape[1] * var_test_data.shape[2])\n",
    "    input_test_data.append(var_test_data)\n",
    "input_train_data = np.concatenate(input_train_data, axis=1)\n",
    "input_train_data = np.array(input_train_data)\n",
    "input_test_data = np.concatenate(input_test_data, axis=1)\n",
    "input_test_data = np.array(input_test_data)\n",
    "\n",
    "# normalize (z-score) the reanalysis data, for use in downscaling methods\n",
    "train_data, input_means, input_stdevs = utilities.normalize_climate_data(input_train_data)\n",
    "test_data, input_test_means, input_test_stdevs = utilities.normalize_climate_data(input_test_data,\n",
    "                                                                                  means=input_means,\n",
    "                                                                                  stdevs=input_stdevs)\n",
    "hist, rean_precip_train = utilities.remove_missing(train_hist, rean_precip_train)\n",
    "train_hist, train_data = utilities.remove_missing(train_hist, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee85ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section 4: initialize the different methods\n",
    "som = som_downscale.som_downscale(som_x=7, som_y=5, batch=512, alpha=0.1, epochs=50)\n",
    "rf_two_part = correction_downscale_methods.two_step_random_forest()\n",
    "random_forest = sklearn.ensemble.RandomForestRegressor()\n",
    "qmap = correction_downscale_methods.quantile_mapping()\n",
    "linear = sklearn.linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b49f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section 5: train and generate outputs\n",
    "som.fit(train_data, train_hist, seed=1)\n",
    "random_forest.fit(train_data, train_hist)\n",
    "rf_two_part.fit(train_data, train_hist)\n",
    "linear.fit(train_data, train_hist)\n",
    "qmap.fit(rean_precip_train, train_hist)\n",
    "\n",
    "# generate outputs from the test data\n",
    "som_output = som.predict(test_data)\n",
    "random_forest_output = random_forest.predict(test_data)\n",
    "rf_two_part_output = rf_two_part.predict(test_data)\n",
    "linear_output = linear.predict(test_data)\n",
    "qmap_output = qmap.predict(rean_precip_test)\n",
    "\n",
    "# generate outputs from the train data for comparision\n",
    "som_train_output = som.predict(train_data)\n",
    "random_forest_train_output = random_forest.predict(train_data)\n",
    "rf_two_part_train_output = rf_two_part.predict(train_data)\n",
    "linear_train_output = linear.predict(train_data)\n",
    "qmap_train_output = qmap.predict(rean_precip_train)\n",
    "\n",
    "# Include the reanalysis precipitation as an undownscaled comparison\n",
    "names = ['SOM', 'Random Forest', 'RF Two Part', 'Linear', 'Qmap', 'NCEP']\n",
    "outputs = [som_output, random_forest_output, rf_two_part_output, linear_output, qmap_output, rean_precip_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: SOM specific analysis plots, heatmap and variable grids\n",
    "freq, avg, dry = som.node_stats()\n",
    "#The heatmap shows the frequency of each node of the SOM in the test set\n",
    "ax = som.heat_map(train_data, annot=avg)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n",
    "i = 0\n",
    "index_range = (window * 2 + 1) ** 2\n",
    "for var in input_vars:\n",
    "    start_index = i * index_range\n",
    "    end_index = (i + 1) * index_range\n",
    "    fig, ax, cbar = som.plot_nodes(weights_index=(start_index, end_index), means=input_means[start_index:end_index],\n",
    "                                   stdevs=input_stdevs[start_index:end_index], cmap='bwr')\n",
    "    for axis in ax.flatten():\n",
    "        axis.set_xticks([])\n",
    "        axis.set_yticks([])\n",
    "\n",
    "    for axis, col in zip(ax[-1], range(0, som.som_x)):\n",
    "        axis.set_xlabel(col, size='large')\n",
    "    for axis, row in zip(ax[:, 0], range(0, som.som_y)):\n",
    "        axis.set_ylabel(row, rotation=0, size='large')\n",
    "    #The variable plots show the value of the variable in each of the SOM nodes \n",
    "    fig.suptitle(var)\n",
    "    units = {'air': '(K)', 'rhum': '(%)', 'uwnd': r'(ms$^{-1}$)', 'vwnd': r'(ms$^{-1}$)', 'hgt': '(m)'}\n",
    "    cbar.set_label(var.capitalize() + ' ' + units[var], rotation='horizontal', labelpad=20)\n",
    "    plt.show()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section 7: Skill metric scores for each method\n",
    "scores = {}\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "i = 0\n",
    "for output in outputs:\n",
    "    noNan_test_hist, noNan_output = utilities.remove_missing(test_hist, output)\n",
    "    pdf_score = distribution_tests.pdf_skill_score(noNan_output, noNan_test_hist)\n",
    "    ks_stat, ks_probs = distribution_tests.ks_testing(noNan_output, noNan_test_hist)\n",
    "    rmse = sklearn.metrics.mean_squared_error(noNan_test_hist, noNan_output, squared=False)\n",
    "    bias = error_metrics.calc_bias(noNan_output, noNan_test_hist)\n",
    "\n",
    "    print(names[i], round(pdf_score, 3), round(ks_stat, 2), round(rmse, 2), round(bias, 2))\n",
    "    scores[names[i]] = [round(pdf_score, 3), round(ks_stat, 2), round(rmse, 2), round(bias, 2)]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fd034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section 8: Plots comparing the outputs: KDE, autocorrelation, and histogram\n",
    "fig, ax = plotters.plot_kde(outputs, names, test_hist, scores, downscaling_target)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plotters.plot_autocorrelation(outputs, names, test_hist)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plotters.scatter_plot(outputs, names, test_hist)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plot_bargraph(outputs, names, test_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ccda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Section 9: Climdex values for each output\n",
    "#Note that the climdex values assume the outputs are a given number of full years, so some of the results will not make sense for seasonal models\n",
    "for i in range(len(outputs)):\n",
    "    climdex_values, func_list = climdex.calc_climdex(outputs[i], downscaling_target, reference_data = train_hist)\n",
    "    print()\n",
    "    print(names[i])\n",
    "    climdex.print_indices(climdex_values, func_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
